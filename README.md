# ğŸ“˜ RAG Document Question Answering System

A **Retrieval-Augmented Generation (RAG)** application that allows users to upload a document, ask questions, and receive answers grounded strictly in the documentâ€™s content.

This project demonstrates document chunking, embeddings, FAISS vector search, LLM answering, and grounding enforcement.

---

## ğŸ“‚ Project Structure

```
app.py      â†’ Core RAG logic
llm.py      â†’ OpenRouter LLM API integration
prompt.py   â†’ Prompt template for grounded answering
ui.py       â†’ Streamlit user interface
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Install dependencies

```
uv sync
```

### 2ï¸âƒ£ Set API Key

Create a `.env` file in the project root:

```
OPENROUTER_API_KEY=YOUR_KEY
```

### 3ï¸âƒ£ Launch the app

```
streamlit run ui.py
```

---

## âœ… Models Used

### ğŸ”¹ Embedding Model

**Model:** `sentence-transformers/all-MiniLM-L6-v2`

**Why this model?**

* Lightweight & fast
* Produces 384-dimensional embeddings
* Strong semantic understanding for QA
* Runs locally (no internet dependency)
* Ideal for academic & practical RAG systems

Embeddings are stored in **FAISS** for efficient retrieval.

---

### ğŸ”¹ LLM

**Provider:** OpenRouter
**Model:** `openai/gpt-4o-mini`

**Why this model?**

* Cost-efficient / often free
* Reliable contextual reasoning
* Works well with retrieval prompts
* Balanced accuracy & performance

The LLM only receives retrieved chunks â€” never the full document â€” ensuring **grounded answers only**.

---

## ğŸ§© RAG Pipeline

### 1ï¸âƒ£ Text Extraction

* PDFs â†’ processed with **PyPDF2**
* Other supported text formats â†’ read directly

---

### 2ï¸âƒ£ Chunking

```
chunk_size = 1000
chunk_overlap = 200
```

**Overlap** helps preserve continuity across chunks.

---

### 3ï¸âƒ£ Embedding & Vector Storage

* Chunks embedded using MiniLM
* Stored in **FAISS vector store**
* Enables fast local semantic search

---

### 4ï¸âƒ£ Semantic Retrieval

When a question is asked:

1ï¸âƒ£ Query embedded
2ï¸âƒ£ FAISS retrieves relevant chunks
3ï¸âƒ£ Top-k chunks returned (typically k = 3â€“5)

Optional enhancements:

* Max-Marginal-Relevance to avoid duplicate chunks
* Similarity threshold to reject weak matches

---

## ğŸ§  Answer Generation (Grounded Only)

Prompt enforces grounding:

```
You are an assistant answering strictly using the provided context.
If the answer is not present, reply: "context not available".

Context:
<retrieved document chunks>

Question:
<user query>
```