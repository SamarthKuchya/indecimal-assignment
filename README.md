# üìò RAG Document Question Answering System

This project implements a **Retrieval-Augmented Generation (RAG)** system that allows users to upload a document, ask questions, and receive answers grounded strictly in the content of that document. It demonstrates document chunking, embeddings, FAISS vector search, LLM answering, and grounding enforcement.

---

### File structure

app.py - contains all the logic
llm.py - code to call openrouter api
prompt.py - contains prompt
ui.py - contains streamlit ui

### Run the app
To run simply excecute '''uv sync'''
create a .env file with '''OPENROUTER_API_KEY=KEY'''
then '''streamlit run ui.py'''

## ‚úÖ Models Used

### üîπ Embedding Model

**Model:** `sentence-transformers/all-MiniLM-L6-v2`

**Reason for choice**

* Lightweight and fast
* Produces 384-dimensional sentence embeddings
* Strong semantic understanding for QA tasks
* Works locally (no internet dependency)
* Ideal for academic RAG implementations

This model converts document chunks into vector embeddings which are stored and used for retrieval.

---

### üîπ LLM

**Provider:** OpenRouter
**Model:** `openai/gpt-4o-mini`

**Reason for choice**

* Free / cost-efficient
* Reliable contextual reasoning
* Works well with retrieval-based prompts
* Good balance of speed and accuracy

The LLM never sees the entire document ‚Äî only retrieved relevant chunks ‚Äî ensuring grounded responses.

---

## üß© Document Chunking & Retrieval Pipeline

### 1Ô∏è‚É£ Text Extraction

* PDF files ‚Üí processed using **PyPDF2**
* Other supported text formats ‚Üí read directly

---

### 2Ô∏è‚É£ Chunking

Documents are split into chunks using:

```
chunk_size = 1000
chunk_overlap = 200
```

**Why overlap?**
To preserve continuity across chunks and avoid losing important context.

---

### 3Ô∏è‚É£ Embedding + Vector Storage

* Each chunk is embedded using `all-MiniLM-L6-v2`
* Stored in a **FAISS vector store**

FAISS enables fast and efficient semantic similarity search on local machine (no external vector DB).

---

### 4Ô∏è‚É£ Semantic Retrieval

When a user asks a question:
1Ô∏è‚É£ Query is embedded
2Ô∏è‚É£ FAISS finds most relevant document chunks
3Ô∏è‚É£ Top-k chunks (typically k = 3‚Äì5) are retrieved

Optional improvements applied:

* Max-Marginal-Relevance retrieval to avoid duplicate chunks
* Similarity threshold to reject weak matches

---

## üß† LLM Answer Generation

The LLM is not allowed to use outside knowledge.

A structured prompt is sent:

```
You are an assistant answering strictly using the provided context.
If the answer is not present, reply: "context not available".

Context:
<retrieved document chunks>

Question:
<user query>
```

The model then generates an answer **only based on retrieved content**.

---
### ‚úî Transparency & Explainability

The system clearly displays:
1Ô∏è‚É£ Retrieved Context Chunks used
2Ô∏è‚É£ Final LLM Answer

---
